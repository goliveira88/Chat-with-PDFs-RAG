{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","machine_shape":"hm","authorship_tag":"ABX9TyPKt2w1JWTMGTftI436xsR6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#**Using Retrieval Augmented Generation (RAG) to chat with pdf**"],"metadata":{"id":"V0jsji9S3eyX"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"sjvkfWk2EAWs"},"outputs":[],"source":["!curl -fsSL https://ollama.com/install.sh | sh"]},{"cell_type":"code","source":["#requirements\n","!pip install -q langchain_community\n","!pip install -q langchain_openai\n","!pip install -q langchain\n","!pip install -q langchain[docarray]\n","!pip install -q pydantic==1.10.8\n","!pip install -q pypdf\n","!pip install -q PyPDF2\n","!pip install -q chromadb\n","!pip install -U sentence-transformers"],"metadata":{"id":"RJor4JMgpEhe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#**Using Ollama**\n","`Ollama` is a Large Language Model (LLM) that can be running locally. After instantiate the model using terminal you can load the model you want [Here](https://ollama.com/library)."],"metadata":{"id":"b69zMs9wFlIS"}},{"cell_type":"code","source":["from langchain_community.llms import Ollama"],"metadata":{"id":"3Mlra05WqUHw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_name = 'llama2:13b'"],"metadata":{"id":"_DL82-6LUhi6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Ollama(model=model_name)"],"metadata":{"id":"dWsIiftqrUhu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model"],"metadata":{"id":"2tp2_M6_riaH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714785725995,"user_tz":180,"elapsed":4,"user":{"displayName":"Gabriel Oliveira de Sousa","userId":"16988133333317490822"}},"outputId":"d5556426-2dc6-4061-8dfe-bfc5eb398d09"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Ollama(model='llama2:13b')"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["print(model.invoke(\"What is Retrieval Augmented Generation (RAG)?\"))"],"metadata":{"id":"GjmoxJ-QrkAd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_community.vectorstores import DocArrayInMemorySearch\n","import docarray"],"metadata":{"id":"OiN8vTOI7vY5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#**Load Document**"],"metadata":{"id":"TF59XwSRlJBe"}},{"cell_type":"code","source":["# Load and process the text files\n","from langchain.document_loaders import (TextLoader,\n","                                        PyPDFLoader,\n","                                        DirectoryLoader)\n","\n","def load_documents(data_path: str):\n","    loader = DirectoryLoader(data_path, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n","    documents = loader.load()\n","    return documents"],"metadata":{"id":"ed-Q2aO60Df9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_path = '/content/'\n","documents = load_documents(data_path)"],"metadata":{"id":"o7-8RL_EgA-k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(documents)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3eR1lBEl1sVT","executionInfo":{"status":"ok","timestamp":1714791088765,"user_tz":180,"elapsed":10,"user":{"displayName":"Gabriel Oliveira de Sousa","userId":"16988133333317490822"}},"outputId":"0d40b121-eb66-4cf0-b262-3fc69bf09499"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["54"]},"metadata":{},"execution_count":55}]},{"cell_type":"code","source":["#splitting a document into chunks\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.schema.document import Document\n","\n","def split_documents(documents: list[Document]):\n","    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,\n","                                                   chunk_overlap=100)\n","    return text_splitter.split_documents(documents)"],"metadata":{"id":"lFVJ-2oDtxSZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["texts = split_documents(documents)"],"metadata":{"id":"I_aIMMCT_HCM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(texts)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"42GSbMFl2D3H","executionInfo":{"status":"ok","timestamp":1714791098818,"user_tz":180,"elapsed":12,"user":{"displayName":"Gabriel Oliveira de Sousa","userId":"16988133333317490822"}},"outputId":"25566a94-60ae-40fc-adbb-3f5ab7e971eb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["345"]},"metadata":{},"execution_count":58}]},{"cell_type":"code","source":["texts[:2]"],"metadata":{"id":"i681sl0S2QTk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#**Word Embedding Model**\n","* Word Embedding is a technique in Natural Language Processing (NLP) that represent words or phrases with a number vector, capturing semantics and caracteristics of them. The \"meaning\" of a word or a phrase is represented by a point in a vector space of N dimensions. So if two words or phrase are close in this space, they have a closer \"meaning\".\n","\n","* The function `get_embedding()` uses two model to load embedding model, `OllamaEmbeddings` and `HuggingFaceEmbeddings`.\n","    * You can find HuggingFace embedding models on [huggingface](https://huggingface.co/spaces/mteb/leaderboard)\n","    * You can find Ollama embedding models on [ollama](https://ollama.com/search?q=embedding%20models&p=1)\n","\n","        *Note: Below there are some embedding models which can be used.*\n","\n","\n","\n"],"metadata":{"id":"hEuSzpZS5kLR"}},{"cell_type":"code","source":["# Ollama embeddings model\n","# Below there are four embedding models from Ollama\n","\n","# !ollama pull all-minilm\n","# !ollama pull mxbai-embed-large\n","# !ollama pull nomic-embed-text\n","!ollama run avr/sfr-embedding-mistral:q4_k_m"],"metadata":{"id":"2yOO16WT8QUH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# HuggingFace embeddings model\n","# Below there are two embedding models from HuggingFace\n","\n","# model_name = \"mixedbread-ai/mxbai-embed-large-v1\"\n","# model_name = \"sentence-transformers/all-mpnet-base-v2\""],"metadata":{"id":"zIbntYTQ8XCP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Embedding model\n","from langchain_community.embeddings import OllamaEmbeddings, HuggingFaceEmbeddings\n","\n","def get_embedding(model_name: str, emb_model: str):\n","\n","    if emb_model == 'Ollama':\n","        embedding = OllamaEmbeddings(model=model_name)\n","    elif emb_model == 'HuggingFace':\n","        embedding = HuggingFaceEmbeddings(model_name=model_name)\n","\n","    return embedding"],"metadata":{"id":"rgfUmx8-LNZX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embedding = get_embedding(model_name='avr/sfr-embedding-mistral:q4_k_m', emb_model='Ollama')"],"metadata":{"id":"zrnPnI8hDtUu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#**Vector Database**\n","* Vector database is a storage of vector embedding and indexes for retrieval and similarity search. Vector database comes to help when dealing with fine-tuning in Large Language Model (LLM), because of the coast behind it. Instead of training LLMs with new information, vector database can be use, together with LLMs, as context retrieval. Thereby an external information (was not seemed by the LLMs) can be stored in a vector database through vector embedding and can be retrieve by a query.  \n","\n","* There are several vector database, between them are:\\\n","  [ChromaDB](https://www.trychroma.com/)\\\n","  [Pinecone](https://www.pinecone.io/)\\\n","  [Qdrant](https://qdrant.tech/)\n","  \n","  *Note*: \\\n","  - *For the top 5 database see: [datacamp](https://www.datacamp.com/blog/the-top-5-vector-databases)*\n","  - *For more information about vector database see: [cloudflare](https://www.cloudflare.com/learning/ai/what-is-vector-database/), [pinecone](https://www.pinecone.io/learn/vector-database/), [medium](https://medium.com/data-and-beyond/vector-databases-a-beginners-guide-b050cbbe9ca0) and [amazon](https://aws.amazon.com/what-is/vector-databases/?nc1=h_ls).*"],"metadata":{"id":"7DRpDxRfhdUF"}},{"cell_type":"code","source":["# Embed and store the texts\n","# Supplying a persist_directory will store the embeddings on disk\n","from langchain.vectorstores import Chroma\n","from langchain.schema.document import Document\n","from langchain_community import embeddings\n","\n","def chroma_db(chunks: list[Document], embedding: embeddings, persist_directory: str):\n","\n","    vectordb = Chroma.from_documents(documents=texts,\n","                                     embedding=embedding,\n","                                     persist_directory=persist_directory)\n","\n","    # persiste the db to disk\n","    vectordb.persist()\n","    vectordb = None\n","\n","    # Now we can load the persisted database from disk, and use it as normal.\n","    vectordb = Chroma(persist_directory=persist_directory,\n","                      embedding_function=embedding)\n","\n","    return vectordb"],"metadata":{"id":"EhzGbAa_gLXa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vectordb_ = chroma_db(chunks=texts,\n","                      embedding=embedding,\n","                      persist_directory='db_')"],"metadata":{"id":"5QEooqxqoC5N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["retriever_ = vectordb_.as_retriever()"],"metadata":{"id":"qD89S1LKgxJt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#**Prompt and Query**"],"metadata":{"id":"W_X0VidVKiLc"}},{"cell_type":"code","source":["#Prompt\n","from langchain.prompts import PromptTemplate\n","\n","template = \"\"\"\n","Answer the question based on the context below. If you can't\n","answer the question, reply \"I don't know\".\n","\n","Context: {context}\n","\n","Quastion: {question}\n","\"\"\"\n","\n","prompt = PromptTemplate.from_template(template)\n","prompt.format(context=\"Here is some context\",\n","              question=\"Here is a question\")"],"metadata":{"id":"ZCvvWB3AKivs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Query\n","from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n","\n","chain_ = ({\"context\": retriever_, \"question\": RunnablePassthrough()}\n","          | prompt\n","          | model)\n","\n","print(chain_.invoke(\"\"))"],"metadata":{"id":"QdxglVz6hGzi"},"execution_count":null,"outputs":[]}]}